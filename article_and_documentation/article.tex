\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Playing Yahtzee with deep reinforcement learning\\- a systematic comparison of different approaches}


\author{
  Markus T.~Dutschke\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Erlangen, Germany\\
  \texttt{post at markusdutschke dot de} \\
  %% examples of more authors
   \And
 Elias D.~Striatum \\
  Department of Electrical Engineering\\
  Mount-Sheikh University\\
  Santa Narimana, Levand \\
  \texttt{stariate@ee.mount-sheikh.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle

\begin{abstract}
In this paper we present an open source Q-learning algorithm for the dice game yahtzee.
We implemented a variation of the Q-learning algorithm as used by Mnih \cite{mnih13},
which he used for playing Atari games.
The specific obstacles of yahtzee are thereby
to handle two different types of possible actions:
  1) choose what dice to re-roll;
  2) choose a category on the score board,
the significantly larger number possible actions of type 1 compared to an Atari game controller
and
the randomness in the response of the game to the players actions of type 1.
By presenting different implementations of increasing complexity,
we give the reader an overview of different concepts to improve the performance of Q-learning
for certain situations and evaluate their performance in the specific use case.
Among those concepts are different exploration strategies, concepts to handle randomness and
a technique for the efficient handling of the two decision types.
The most successful implementation achieves superhuman performance within a few thousand training cycles.
\end{abstract}


% keywords can be removed
\keywords{Q-learning \and neural networks \and exploration strategies \and replay memory}


\section{Introduction}

The complete source code of this project is publicly available at
\begin{center}
  \url{https://github.com/markusdutschke/yahtzee}
\end{center}

Since Mnihs famous publication 'Playing Atari with deep reinforcement learning' \cite{mnih13}
strong research interest has evolved around the possibilities of Q-learning in combination with neural networks.
Thereby computer and board games turned out to be an excellent playground for this research,
due to their complex character, their easy reproducibility and the clear definition of the systems rules.

ToDo: One should mention some historic achievments with the corresponding machine learning technology here.
This should include
- gackgammon (ibm, temporal time difference)
- go (deepMind, ?)
and many more (chesss?, poker?, doom?).

Solving these puzzles often paved the path for more complex applications like
ToDo:
- thermomix, which evolved out of the solution for fruit ninja (this is false and just an example)
- more eamples of this structure

Especially the dice game Yahtzee has a set of interesting properties,
which makes it an highly interesting test system for our purpose:
\begin{itemize}
\item Yahtzee is a broadly known game.
This makes it easy for many researchers to evaluate a certain decision of the algorithm.
\item Even after several hundred games, Yahtzee is still challenging for a human player.
It thereby represents a challenge, which goes bejond the development of a few best practice strategies.
\item There is a mixture of randomness and strategy involved.
This makes it an interesting application which combines the reproducible domain of games
with the influence of statistical uncertainty in real-life applications.
\item Yahtzee is exactly solvable.
The solution is far beyond the human abilities but can be used to evaluate the performance
of the Q-learning implementation.
\end{itemize}


\section{The dice game Yahtzee}

\subsection{Rules}

\subsection{Implementation}
The code is executed by calling \emph{main.py} in the root folder.
For an extended functionality, there are several functions implemented, which are however not called.
These functions is a good starting point of playing around with the code.

The complete game logic can be found in \emph{lib/yahtzee.py}.

The class \emph{Dice}, thereby encodes a set of zero to five dice.
The \emph{roll} method is used to re-roll one or more dice of a given set.
The \emph{keep} method reduces the dice in a set to the ones, which shal be kept.
This method is used to compactly encode game situations, 
where certain dice shall be re-rolled and hence their values are irrelevant.

The score board is encoded in \emph{ScoreBoard}.
The class-method \emph{get\_cat\_points} returns the number of points a player gets,
when assigning a certain dice combination to a category.
\emph{stat\_cat\_score} calculates the exact expectation values and their standard error for each category
based on a set of dice. Using a dice configuration of five dice, is the trivial use-case.
For dice configurations with less then five dice, all combinations of the unspecified dice are looped over.
The method \emph{add} is used to assign a set of dice to a category on the score board.
Open categories are accessed by the methods \emph{mask}, \emph{open\_cats\_mask} (todo: probably redundnat to mask)
and \emph{open\_cats}.
The sum of a score board is evaluated by the methods \emph{getUpperSum}, \emph{getLowerSum} and \emph{getSum}.

A game can be played by using the class \emph{Game}.
Thereby a player object is given for initialization.
A \emph{player}-object supplies two methods: \emph{choose\_reroll} and \emph{choose\_cat}.
The \emph{Game}-class is calling the \emph{autoplay}-method on initialization.
The game is started with an empty score board and five dice values.
From then on the suitable methods of \emph{player} are called and the resulting course of the game is simulated in alternating order.
A protocol of the game is found in the attribute \emph{log},
which can be most easily accessed by a string-cast of Game.
This represents all dice configurations and the players decisions together with the complete score board.
The internal evaluation for the decisions can be accessed by setting debug to 1.
This feature is not supported for all players.

Different player types are defined in \emph{lib/bot.py}.
They all descend from the class \emph{AbstractPlayer},
which abstractly defines the two mandatory methods \emph{choose\_reroll} and \emph{choose\_cat}.
Further a method \emph{benchmark} is implemented, 
which calculates the mean and standard deviation of playing \emph{nGames} (usually 100)
with this player.
For reproducibility the seed for random numbers is usually fixed
(constant \emph{BENCHMARK\_SEED} in main.py).

One subclass, which is important to mention here, is \emph{TemporaryPlayer}.
This class represents a player,
which is initiated by custom re-roll and choose category functions.
This class fulfills mainly the purpose of clean code and 
is used to simulate a game, when just the two functions are available.

The other subclasses of \emph{AbstractPlayer} are implementation of 
specific players/strategies as described in section \ref{sec_impl}.

\subsection{Solutions and heuristics}


\section{Q-learning}
This chapter contains all the theoretical background of the code.
\subsection{Background}
\subsection{Handling the two decision types}
\subsection{Information encoding}
\subsection{Exploration}
\subsection{Concepts to handle a stochastic system response}

\section{Implementations}
\label{sec_impl}
In the following we describe the implementation of different players in \emph{lib/bot.py}.

\subsection{Naive Implementations}
- random implementation
- greedy implementation with and without re-roll
\subsection{AI player Version 0}
\subsection{AI player Version 1}
\subsection{AI player Version 2}

\section{Benchmark}
In this chapter the benefit of different Q-learning concepts are quantitatively benchmarked.
The implementation of these benchmarks can be found in the functions bench... in main.py with player implementations in botBench.py
\subsection{Information encoding}
Different encodings. Not yet sure, what to compare.
Maybe: rgrSC with 
- 13 inputs (-1 for empty, otherwise score)
- 26 inputs (first 13: 0 for empty, second 13: 0 or 1 for empty and used)
- a good encoding (check maybe player v2)
\subsection{Exploration}
- epsilon greedy
- softmax
- minMaxRat
\subsection{Concepts to handle a stochastic system response}
- implicitly in MLP regressor (v0)
- explicitly in mlprgr with pretraining and benchmarking (this is v1)
- exactly by lookup table (v2)


\section{Conclusion}
Collection of key facts, whatever turned out to bring the most significant improvement.



\section{NOW FOLLOWS THE TEMPLATE}

\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrt}  
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\end{thebibliography}


\end{document}

\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Playing Yahtzee with deep reinforcement learning\\- a systematic comparison of different approaches}


\author{
  Markus T.~Dutschke\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  auXolutions\\
  Erlangen, Germany\\
  \texttt{markus.dutschke@auXolutions.de} \\
  %% examples of more authors
   \And
 Elias D.~Striatum \\
  Department of Electrical Engineering\\
  Mount-Sheikh University\\
  Santa Narimana, Levand \\
  \texttt{stariate@ee.mount-sheikh.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle

\begin{abstract}
In this paper we present an open source Q-learning algorithm for the dice game yahtzee.
We implemented a variation of the Q-learning algorithm as used by Mnih \cite{mnih13},
which he used for playing Atari games.
The specific obstacles of yahtzee are thereby
to handle two different types of possible actions:
  1) choose what dice to re-roll;
  2) choose a category on the score board,
the significantly larger number possible actions of type 1 compared to an Atari game controller
and
the randomness in the response of the game to the players actions of type 1.
By presenting different implementations of increasing complexity,
we give the reader an overview of different concepts to improve the performance of Q-learning
for certain situations and evaluate their performance in the specific use case.
Among those concepts are different exploration strategies, concepts to handle randomness and
a technique for the efficient handling of the two decision types.
The most successful implementation achieves superhuman performance within a few thousand training cycles.
\end{abstract}


% keywords can be removed
\keywords{Q-learning \and neural networks \and exploration strategies \and replay memory}


\section{Introduction}

The complete source code of this project is publicly available at
\begin{center}
  \url{https://github.com/markusdutschke/yahtzee}
\end{center}

Since Mnihs famous publication 'Playing Atari with deep reinforcement learning' \cite{mnih13}
strong research interest has evolved around the possibilities of Q-learning in combination with neural networks.
Thereby computer and board games turned out to be an excellent playground for this research,
due to their complex character, their easy reproducibility and the clear definition of the systems rules.

Especially the dice game Yahtzee has a set of interesting properties,
which makes it especially suitable for a Q-learning test system.
%\footnote{In fact, Yahtzee was chosen, due to the authors personal interest.
%A few of those properties appears later on.}
\begin{itemize}
\item Yahtzee is a broadly known game.
\item Even after several hundred games, Yahtzee is still challenging for human player.
\item There is a mixture of randomness and strategy involved.
\item Yahtzee is exactly solved, but this is far beyond the score of the abilities of a human brain.
\end{itemize}


\section{The dice game Yahtzee}
\subsection{Rules}
\subsection{Code}
Relate the rule with the different classes in the code.
Especially: Dice, ScoreBoard, Game and AbstractPlayer
\subsection{Existing literature}

\section{Q-learning}
This chapter contains all the theoretical background of the code.
\subsection{Background}
\subsection{Handling the two decision types}
\subsection{Information encoding}
\subsection{Exploration}
\subsection{Concepts to handle a stochastic system response}

\section{Implementations}
This chapter is just a description of the different implementations and their performance.
\subsection{Naive Implementations}
- random implementation
- greedy implementation with and without re-roll
\subsection{AI player Version 0}
\subsection{AI player Version 1}
\subsection{AI player Version 2}

\section{Benchmark}
In this chapter the benefit of different Q-learning concepts are quantitatively benchmarked.
The implementation of these benchmarks can be found in the functions bench... in main.py with player implementations in botBench.py
\subsection{Information encoding}
Different encodings. Not yet sure, what to compare.
Maybe: rgrSC with 
- 13 inputs (-1 for empty, otherwise score)
- 26 inputs (first 13: 0 for empty, second 13: 0 or 1 for empty and used)
- a good encoding (check maybe player v2)
\subsection{Exploration}
- epsilon greedy
- softmax
- minMaxRat
\subsection{Concepts to handle a stochastic system response}
- implicitly in MLP regressor (v0)
- explicitly in mlprgr with pretraining and benchmarking (this is v1)
- exactly by lookup table (v2)


\section{Conclusion}
Collection of key facts, whatever turned out to bring the most significant improvement.



\section{NOW FOLLOWS THE TEMPLATE}

\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrt}  
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\end{thebibliography}


\end{document}

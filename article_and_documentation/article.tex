\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{Playing Yahtzee with deep reinforcement learning\\- a systematic comparison of different approaches}


\author{
  Markus T.~Dutschke\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Erlangen, Germany\\
  \texttt{post at markusdutschke dot de} \\
  %% examples of more authors
   \And
 Elias D.~Striatum \\
  Department of Electrical Engineering\\
  Mount-Sheikh University\\
  Santa Narimana, Levand \\
  \texttt{stariate@ee.mount-sheikh.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle

\begin{abstract}
In this paper we present an open source Q-learning algorithm for the dice game yahtzee.
We implemented a variation of the Q-learning algorithm as used by Mnih \cite{mnih13},
which he used for playing Atari games.
The specific obstacles of yahtzee are thereby
to handle two different types of possible actions:
  1) choose what dice to re-roll;
  2) choose a category on the score board,
the significantly larger number possible actions of type 1 compared to an Atari game controller
and
the randomness in the response of the game to the players actions of type 1.
By presenting different implementations of increasing complexity,
we give the reader an overview of different concepts to improve the performance of Q-learning
for certain situations and evaluate their performance in the specific use case.
Among those concepts are different exploration strategies, concepts to handle randomness and
a technique for the efficient handling of the two decision types.
The most successful implementation achieves superhuman performance within a few thousand training cycles.
\end{abstract}


% keywords can be removed
\keywords{Q-learning \and neural networks \and exploration strategies \and replay memory}


\section{Introduction}

The complete source code of this project is publicly available at
\begin{center}
  \url{https://github.com/markusdutschke/yahtzee}
\end{center}

Since Mnihs famous publication 'Playing Atari with deep reinforcement learning' \cite{mnih13}
strong research interest has evolved around the possibilities of Q-learning in combination with neural networks.
Thereby computer and board games turned out to be an excellent playground for this research,
due to their complex character, their easy reproducibility and the clear definition of the systems rules.

ToDo: One should mention some historic achievments with the corresponding machine learning technology here.
This should include
- gackgammon (ibm, temporal time difference)
- go (deepMind, ?)
and many more (chesss?, poker?, doom?).

Solving these puzzles often paved the path for more complex applications like
ToDo:
- thermomix, which evolved out of the solution for fruit ninja (this is false and just an example)
- more eamples of this structure

Especially the dice game Yahtzee has a set of interesting properties,
which makes it an highly interesting test system for our purpose:
\begin{itemize}
\item Yahtzee is a broadly known game.
This makes it easy for many researchers to evaluate a certain decision of the algorithm.
\item Even after several hundred games, Yahtzee is still challenging for a human player.
It thereby represents a challenge, which goes bejond the development of a few best practice strategies.
\item There is a mixture of randomness and strategy involved.
This makes it an interesting application which combines the reproducible domain of games
with the influence of statistical uncertainty in real-life applications.
\item Yahtzee is exactly solvable.
The solution is far beyond the human abilities but can be used to evaluate the performance
of the Q-learning implementation.
\end{itemize}


\section{The dice game Yahtzee}

\subsection{Rules}

\subsection{Implementation}
The code is executed by calling \emph{main.py} in the root folder.
For an extended functionality, there are several functions implemented, which are however not called.
These functions is a good starting point of playing around with the code.

The complete game logic can be found in \emph{lib/yahtzee.py}.

The class \emph{Dice}, thereby encodes a set of zero to five dice.
The \emph{roll} method is used to re-roll one or more dice of a given set.
The \emph{keep} method reduces the dice in a set to the ones, which shal be kept.
This method is used to compactly encode game situations, 
where certain dice shall be re-rolled and hence their values are irrelevant.

The score board is encoded in \emph{ScoreBoard}.
The class-method \emph{get\_cat\_points} returns the number of points a player gets,
when assigning a certain dice combination to a category.
\emph{stat\_cat\_score} calculates the exact expectation values and their standard error for each category
based on a set of dice. Using a dice configuration of five dice, is the trivial use-case.
For dice configurations with less then five dice, all combinations of the unspecified dice are looped over.
The method \emph{add} is used to assign a set of dice to a category on the score board.
Open categories are accessed by the methods \emph{mask}, \emph{open\_cats\_mask} (todo: probably redundnat to mask)
and \emph{open\_cats}.
The sum of a score board is evaluated by the methods \emph{getUpperSum}, \emph{getLowerSum} and \emph{getSum}.

A game can be played by using the class \emph{Game}.
Thereby a player object is given for initialization.
A \emph{player}-object supplies two methods: \emph{choose\_reroll} and \emph{choose\_cat}.
The \emph{Game}-class is calling the \emph{autoplay}-method on initialization.
The game is started with an empty score board and five dice values.
From then on the suitable methods of \emph{player} are called and the resulting course of the game is simulated in alternating order.
A protocol of the game is found in the attribute \emph{log},
which can be most easily accessed by a string-cast of Game.
This represents all dice configurations and the players decisions together with the complete score board.
The internal evaluation for the decisions can be accessed by setting debug to 1.
This feature is not supported for all players.

Different player types are defined in \emph{lib/bot.py}.
They all descend from the class \emph{AbstractPlayer},
which abstractly defines the two mandatory methods \emph{choose\_reroll} and \emph{choose\_cat}.
Further a method \emph{benchmark} is implemented, 
which calculates the mean and standard deviation of playing \emph{nGames} (usually 100)
with this player.
For reproducibility the seed for random numbers is usually fixed
(constant \emph{BENCHMARK\_SEED} in main.py).

One subclass, which is important to mention here, is \emph{TemporaryPlayer}.
This class represents a player,
which is initiated by custom re-roll and choose category functions.
This class fulfills mainly the purpose of clean code and 
is used to simulate a game, when just the two functions are available.

The other subclasses of \emph{AbstractPlayer} are implementation of 
specific players/strategies as described in section \ref{sec_players}.

\subsection{Solutions and heuristics}


\section{Q-learning}
This chapter contains the theoretical background and 
explaines different techniques to achiev a faster convergence and more successfull game strategies.

\subsection{Background}
\subsection{Handling the two decision types}

\subsection{Encoding}
TODO: general remarks about encoding

The current dice configuration is always stored and handled in a sorted manner.
This reduces the complexity of the setup and supports the convergence of the regressors.

In the following, we explain a set of possible encodings for the different regressors.
We will refer to these in section \ref{sec_players},
when discussing the specific implementation of the different self-learning players.
If the number of features or the encoding are not specified, this means, that there are different implementations used.

For the chose-a-category regressor, we only need to forecast the rest score of the game,
based only on the available categories on the score board and the upper sum (relevant for the bonus).
\begin{itemize}
\item category availability: the availability of each category on the score board; 13 features encoded as 0 or 1
\item bonus: current sum of the upper half of the score board; 1 feature
\end{itemize}
This encoding is equivalent for all regressors of that kind.

For the re-roll decision regressor the following additional encodings turned out to be worth considering:
\begin{itemize}
\item attempt: first or second re-roll attempt; 1 feature encoded as 0 or 1
\item dice: the dice, which are held fixed (i.e. not being re-rolled); 5 features
\item dice histogram: number of dice showing ones, twos, ...; 6 features
\item sum of dice: sum of the values of all dice as entered in the category chance, 1 feature encoded $\mathrm{int} \in [5,30]$.
\item n of a kind: number of different values, which are presented by exactly n dice; 1 feature for each $n \in \{1,2,3,4,5,6\}$
\item helper small straight: some heuristic, intended to represent the chances of a small straight
\item helper large straight: some heuristic, intended to represent the chances of a large straight
\item statistical forecast: exact or heuristic forecast of the expected score in each category
      based on the currently kept dice configuration; 13 features encoded as expectation value and as 0 if not available
\end{itemize}

\subsection{Exploration}
\subsection{Concepts to handle a stochastic system response}

\section{Implementation of classic and ai players}
\label{sec_players}
In the following we describe the implementation of different players in \emph{lib/bot.py}.

\subsection{Naive Implementations}
- random implementation
- greedy implementation with and without re-roll

\subsection{Remarks about AI players}

The AI players have a set of common / similar methods, which shall be explained here quickly.

The \emph{eval\_options\_cat/reroll} method evaluates all possible actions
(eigther choosing a category or choosing dice to re-roll)
and returns an array consisting of the (action, evaluated benefit) tuples for each option.
The method \emph{train} is used to train/configure the regressors based on a nGames training games.
To ensure reproducibility, the random seed is set to the total number of training games for each trainig cicle.
\emph{save} and \emph{load} are used to save and load a previously trained AI player.

The also exists a set to different methods \emph{add\_to\_..RgrMem} and \emph{to\_repMem},
which store the training game experience in the replay memory.
This has some size limitation, which is defined as an arguement of \emph{\_\_init\_\_}.
Based on this, the replay memory is also truncated at a point
(eighter in \emph{truncate\_...} or at the end of \emph{to\_repMem}).

As discussed in \ref{subsec_encoding}, the encoding of the current state is of crucial importance.
For this reason each regressor has a corresponding method \emph{encode\_...},
which just encodes the state of the current game situation (score board and dice)
in the most compact and processible way.
The corresponding property \emph{nFeat\_...} is a constant, 
used in the methods \emph{encode} and \emph{train} and
defines the dimensionality (number of features) of the regressor input.

\subsection{AI player Version 0}
%Performance
This player reaches a maximum average score of 200.1 after 1053 training games
(compare commit d68fc302a929101602ff95c609298ffcf6f86612).

% Functionality
We use two multi layer perceptron regressors (\emph{scrRgr} and \emph{rrRgr}).
Both predict the rest score of the game (i.e. the final score minus the current score).
This concept is updated in version 1.

The encoding for the re-roll decision


\subsection{AI player version 1}
\subsection{AI player version 2}

\section{Benchmark}
In this chapter the benefit of different Q-learning concepts are quantitatively benchmarked.
The implementation of these benchmarks can be found in the functions bench... in main.py with player implementations in botBench.py

\subsection{Direct comparison of the AI players}
Performance plots for version 0-2
\subsection{Information encoding}
Different encodings. Not yet sure, what to compare.
Maybe: rgrSC with 
- 13 inputs (-1 for empty, otherwise score)
- 26 inputs (first 13: 0 for empty, second 13: 0 or 1 for empty and used)
- a good encoding (check maybe player v2)
\subsection{Exploration}
- epsilon greedy
- softmax
- minMaxRat
\subsection{Concepts to handle a stochastic system response}
- implicitly in MLP regressor (v0)
- explicitly in mlprgr with pretraining and benchmarking (this is v1)
- exactly by lookup table (v2)


\section{Conclusion}
Collection of key facts, whatever turned out to bring the most significant improvement.


\pagebreak
\section{END OF ARTICLE - NOW FOLLOWS THE LATEX TEMPLATE}

\section{Headings: first level}
\label{sec:headings}

\lipsum[4] See Section \ref{sec:headings}.

\subsection{Headings: second level}
\lipsum[5]
\begin{equation}
\xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
\end{equation}

\subsubsection{Headings: third level}
\lipsum[6]

\paragraph{Paragraph}
\lipsum[7]

\section{Examples of citations, figures, tables, references}
\label{sec:others}
\lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}


\subsection{Figures}
\lipsum[10] 
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11] 

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
  \label{fig:fig1}
\end{figure}

\subsection{Tables}
\lipsum[12]
See awesome Table~\ref{tab:table}.

\begin{table}
 \caption{Sample table title}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
  \label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
\item Lorem ipsum dolor sit amet
\item consectetur adipiscing elit. 
\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrt}  
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\end{thebibliography}


\end{document}
